# requirements.txt
Flask==2.3.3
tensorflow==2.13.0
pandas==2.1.1
numpy==1.24.3
scikit-learn==1.3.0
requests==2.31.0
APScheduler==3.10.4
Flask-APScheduler==1.13.0
gunicorn==21.2.0

# scripts/train_model.py
"""
Standalone script to train the LSTM model
Usage: python scripts/train_model.py
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.models import PricePredictor
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def main():
    """Train the LSTM model"""
    logger.info("Starting model training...")
    
    predictor = PricePredictor()
    metrics = predictor.train_model()
    
    logger.info("Training completed!")
    logger.info(f"Training RMSE: {metrics['train_rmse']:.4f}")
    logger.info(f"Test RMSE: {metrics['test_rmse']:.4f}")
    logger.info(f"Training MAE: {metrics['train_mae']:.4f}")
    logger.info(f"Test MAE: {metrics['test_mae']:.4f}")

if __name__ == "__main__":
    main()

# scripts/data_collector.py
"""
Script to collect and store historical price data
Usage: python scripts/data_collector.py
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.utils import fetch_price_data
import pandas as pd
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def main():
    """Collect and save historical price data"""
    logger.info("Fetching historical price data...")
    
    # Fetch 1 year of data
    data = fetch_price_data(days=365)
    
    # Create data directory if it doesn't exist
    os.makedirs('data/raw', exist_ok=True)
    os.makedirs('data/processed', exist_ok=True)
    
    # Save raw data
    raw_path = 'data/raw/sol_price_raw.csv'
    data.to_csv(raw_path)
    logger.info(f"Raw data saved to {raw_path}")
    
    # Process and save
    processed_path = 'data/processed/sol_price_data.csv'
    data.to_csv(processed_path)
    logger.info(f"Processed data saved to {processed_path}")
    
    logger.info(f"Data shape: {data.shape}")
    logger.info(f"Date range: {data.index.min()} to {data.index.max()}")

if __name__ == "__main__":
    main()

# scripts/deploy.sh
#!/bin/bash

# Deployment script for AWS EC2
echo "Starting SOL Price Prediction deployment..."

# Update system
sudo apt update && sudo apt upgrade -y

# Install Python and pip
sudo apt install python3 python3-pip python3-venv nginx -y

# Create application directory
sudo mkdir -p /opt/sol-prediction
sudo chown $USER:$USER /opt/sol-prediction

# Clone repository (replace with your repo URL)
cd /opt/sol-prediction
git clone https://github.com/yourusername/sol-price-prediction.git .

# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Create directories
mkdir -p data/raw data/processed data/models logs

# Set environment variables
export FLASK_APP=run.py
export FLASK_ENV=production
export SECRET_KEY=$(python3 -c 'import secrets; print(secrets.token_hex(16))')

# Train initial model
echo "Training initial model..."
python scripts/data_collector.py
python scripts/train_model.py

# Create systemd service
sudo tee /etc/systemd/system/sol-prediction.service > /dev/null << EOF
[Unit]
Description=SOL Price Prediction Flask App
After=network.target

[Service]
Type=simple
User=$USER
WorkingDirectory=/opt/sol-prediction
Environment=PATH=/opt/sol-prediction/venv/bin
Environment=FLASK_APP=run.py
Environment=FLASK_ENV=production
Environment=SECRET_KEY=$SECRET_KEY
ExecStart=/opt/sol-prediction/venv/bin/gunicorn --workers 3 --bind 127.0.0.1:8000 run:app
Restart=always

[Install]
WantedBy=multi-user.target
EOF

# Configure Nginx
sudo tee /etc/nginx/sites-available/sol-prediction > /dev/null << EOF
server {
    listen 80;
    server_name _;

    location / {
        proxy_pass http://127.0.0.1:8000;
        proxy_set_header Host \$host;
        proxy_set_header X-Real-IP \$remote_addr;
        proxy_set_header X-Forwarded-For \$proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto \$scheme;
    }

    location /static {
        alias /opt/sol-prediction/static;
        expires 1y;
        add_header Cache-Control "public, immutable";
    }
}
EOF

# Enable and start services
sudo ln -sf /etc/nginx/sites-available/sol-prediction /etc/nginx/sites-enabled/
sudo rm -f /etc/nginx/sites-enabled/default
sudo nginx -t
sudo systemctl reload nginx
sudo systemctl enable sol-prediction
sudo systemctl start sol-prediction

# Setup log rotation
sudo tee /etc/logrotate.d/sol-prediction > /dev/null << EOF
/opt/sol-prediction/logs/*.log {
    daily
    missingok
    rotate 52
    compress
    notifempty
    create 0640 $USER $USER
}
EOF

# Create cron job for daily model updates
(crontab -l 2>/dev/null; echo "0 2 * * * cd /opt/sol-prediction && /opt/sol-prediction/venv/bin/python scripts/train_model.py >> logs/training.log 2>&1") | crontab -

echo "Deployment completed!"
echo "Application should be running on port 80"
echo "Check status with: sudo systemctl status sol-prediction"
echo "View logs with: sudo journalctl -u sol-prediction -f"

# Dockerfile
FROM python:3.9-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Create directories
RUN mkdir -p data/raw data/processed data/models logs

# Set environment variables
ENV FLASK_APP=run.py
ENV FLASK_ENV=production
ENV PYTHONPATH=/app

# Expose port
EXPOSE 5000

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:5000/ || exit 1

# Run application
CMD ["gunicorn", "--bind", "0.0.0.0:5000", "--workers", "2", "run:app"]

# docker-compose.yml
version: '3.8'

services:
  web:
    build: .
    ports:
      - "5000:5000"
    environment:
      - FLASK_ENV=production
      - SECRET_KEY=your-secret-key-here
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/"]
      interval: 30s
      timeout: 10s
      retries: 3

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./static:/var/www/static
    depends_on:
      - web
    restart: unless-stopped

# nginx.conf
events {
    worker_connections 1024;
}

http {
    upstream app {
        server web:5000;
    }

    server {
        listen 80;
        
        location / {
            proxy_pass http://app;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        location /static {
            alias /var/www/static;
            expires 1y;
            add_header Cache-Control "public, immutable";
        }
    }
}

# .gitignore
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# Virtual Environment
venv/
env/
ENV/

# IDE
.vscode/
.idea/
*.swp
*.swo

# Data and Models
data/raw/*.csv
data/processed/*.csv
data/models/*.h5
data/models/*.pkl

# Logs
logs/*.log
*.log

# Environment Variables
.env
.env.local

# OS
.DS_Store
Thumbs.db

# Jupyter Notebooks
.ipynb_checkpoints/

# Flask
instance/
.webassets-cache

# Coverage
.coverage
htmlcov/

# Testing
.pytest_cache/
.tox/

# MyPy
.mypy_cache/
.dmypy.json
dmypy.json